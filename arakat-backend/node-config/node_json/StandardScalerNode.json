{
  "name": "StandardScaler",
  "id": 21,
  "category": 4,
  "family": 5,
  "incompatible_families": [
    1,
		10,
		11,
		12,
		13
  ],
  "requires_udf": false,
  "is_splitter": false,
  "compatible_with_spark_pipeline": true,
  "compatible_with_stream": false,
  "compatible_stream_output_modes": [
    
  ],
  "produces_model": true,
  "parameter_props": {
    "inputCol": {
      "type": [
        "string"
      ],
      "constraints": [
        
      ],
      "default": "No default value, must be provided"
    },
    "outputCol": {
      "type": [
        "string"
      ],
      "constraints": [
        
      ],
      "default": "No default value, must be provided"
    },
    "withMean": {
      "type": [
        "boolean"
      ],
      "constraints": [
        
      ],
      "default": "False"
    },
    "withStd": {
      "type": [
        "boolean"
      ],
      "constraints": [
        
      ],
      "default": "True"
    }
  },
  "df_constraints": [
  	3,
    10,
    13
  ],
  "import": "from pyspark.ml.feature import StandardScaler",
  "explanation": "Scales all the values in input by subtracting the mean from them(withMean=True) and then dividing to the standard deviation(withStd=True). Creates output column of same type"
}
