{
"name": "DecisionTreeRegressor",
"id": 31,
"category": 7,
"family": 5,
"incompatible_families": [10,11,12,13],
"requires_udf": false,
"is_splitter": false,
"compatible_with_spark_pipeline": true,
"compatible_with_stream": false,
"compatible_stream_output_modes": [],
"produces_model": true,
"parameter_props": {"featuresCol": {"type": ["string"], "constraints":[], "default":"features"}, "labelCol": {"type": ["string"], "constraints":[], "default":"label"}, "predictionCol": {"type": ["string"], "constraints":[], "default":"prediction"},
"maxDepth": {"type": ["int"], "constraints":[22], "default": "5"},
"maxBins": {"type": ["int"], "constraints":[25], "default": "32"},
"minInstancesPerNode": {"type": ["int"], "constraints":[28], "default": "1"},
"impurity": {"type": ["string"], "constraints":[76], "default": "gini"},
"checkpointInterval": {"type": ["int"], "constraints":[34], "default": "10"},
"cacheNodeIds": {"type": ["bool"], "constraints":[], "default": "False"},
"seed": {"type": ["int"], "constraints":[], "default": "None"},
"varianceCol": {"type": ["string"], "constraints":[], "default":"None"}},
"df_constraints": [0,1,4,5,7,10,13],
"import": "from pyspark.ml.regression import DecisionTreeRegressor",
"explanation": "Decision tree learning algorithm for regression. It supports both continuous and categorical features. Added prediction columns to schema. CheckpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. MaxDepth: Maximum depth of the tree. CacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. "
}
